{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cbc00f-5510-4208-bc9f-4f94898a3c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing des images .TIF du dataset lnria Aerial Image Labeling Dataset\n",
    "# Roof Detection / Segmentation\n",
    "\n",
    "5000x5000 px en résolution 0.3 m /px vers 1024x1024 px en résolution 0.6 m /px\n",
    "\n",
    "21/11/21\n",
    "convertir toutes les images du dataset INRIA en 1024 px 0.6 m/px et annotations associées au format VGG Annotation Tool en json \n",
    "\n",
    "Création du fichier via_region_data.json pour stocker toutes les annotations de toutes les images considérées dans l'étude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1ce35c-b04e-4c45-978b-37690e8cecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from osgeo import gdal \n",
    "import json\n",
    "import copy\n",
    "import base64\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39c0f23-18be-448b-8dc8-6a0425c0a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\"\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train', 'images')\n",
    "gt_dir    = os.path.join(dataset_dir, 'train', 'gt')\n",
    "test_dir  = os.path.join(dataset_dir, 'test', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc37f51-f4d1-4ee0-b6e6-1508df3c7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(dataset_dir, 'train', 'gt', 'austin16.tif')\n",
    "!gdalinfo $filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ae114-295c-4f6c-9eed-99044a7d8607",
   "metadata": {},
   "source": [
    "## Convert GeoTIFF to 0.6 m resolution GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce20ac6a-70b7-425e-8f3a-8d1ac976651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(dataset_dir, res_dir, file_name, res=0.6):\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    name, ext = file_name.split('.')\n",
    "    res_path = os.path.join(res_dir, name+'_s.'+ext)\n",
    "    !gdalwarp -tr $res $res $file_path $res_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2427dc-bd98-403b-ad95-520a69535000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = os.path.join(dataset_dir, 'tmpINRIA')\n",
    "if not os.path.isdir(tmp_path):\n",
    "    os.mkdir(tmp_path)\n",
    "    os.mkdir(os.path.join(tmp_path, 'images'))\n",
    "    os.mkdir(os.path.join(tmp_path, 'gt'))\n",
    "\n",
    "lst = os.listdir(train_dir)\n",
    "for i in lst:\n",
    "    if i.startswith('austin') or i.startswith('chicago') or i.startswith('vienna') or i.startswith('tyrol'):\n",
    "        res(train_dir, os.path.join(tmp_path, 'images'), i, res=0.6)\n",
    "        res(gt_dir, os.path.join(tmp_path, 'gt'), i, res=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752b047-e540-417c-ab8c-6bc98836da85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert GeoTIFF to 1024 px side GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b937fe11-f0f1-4c2f-bf2b-d82593113746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetExtent(ds):\n",
    "    \"\"\" Return list of corner coordinates from a gdal Dataset \"\"\"\n",
    "    xmin, xpixel, _, ymax, _, ypixel = ds.GetGeoTransform()\n",
    "    width, height = ds.RasterXSize, ds.RasterYSize\n",
    "    xmax = xmin + width * xpixel\n",
    "    ymin = ymax + height * ypixel\n",
    "    return round(xmin,0), round(xmax,0), round(ymin,0), round(ymax, 0)\n",
    "# https://gis.stackexchange.com/questions/57834/how-to-get-raster-corner-coordinates-using-python-gdal-bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6215ac13-e08e-4081-a130-11fa26356b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dataset_dir, final_dir, file_name):\n",
    "    file_path = os.path.join(dataset_dir, 'images', file_name)\n",
    "    gt_path = os.path.join(dataset_dir, 'gt', file_name)\n",
    "    \n",
    "    name, exte = file_name.split('.')\n",
    "    raster = gdal.Open(file_path)\n",
    "    # print(file_path)\n",
    "    ext = GetExtent(raster)\n",
    "    Xdim, Ydim = raster.RasterXSize, raster.RasterYSize\n",
    "    for i in range(Xdim//1024+1):\n",
    "        for j in range(Ydim//1024+1):\n",
    "            final_name = name[:-2]+ '_'+str(i)+str(j)+'.'+exte\n",
    "            final_path = os.path.join(final_dir, 'images', final_name)\n",
    "            final_gt_path = os.path.join(final_dir, 'gt', final_name)\n",
    "            \n",
    "            if i==Xdim//1024 or j==Ydim//1024:\n",
    "                if i==Xdim//1024 and j!=Ydim//1024:\n",
    "                    nxmin = ext[1] - 1024 * 0.6\n",
    "                    nxmax = ext[1]\n",
    "                    nymin = ext[3] - 1024 * 0.6 * (j+1)\n",
    "                    nymax = ext[3] - 1024 * 0.6 * j\n",
    "                    #pxlim = [Xdim - 1024, Xdim, 0, 1024]\n",
    "                    \n",
    "                elif i!=Xdim//1024 and j==Ydim//1024:\n",
    "                    nxmin = ext[0] + 1024 * 0.6 * i\n",
    "                    nxmax = ext[0] + 1024 * 0.6 * (i+1)\n",
    "                    nymin = ext[2]\n",
    "                    nymax = ext[2] + 1024 * 0.6\n",
    "                    #pxlim = [0, 1024, Ydim - 1024, Ydim]\n",
    "                    \n",
    "                elif i==Xdim//1024 and j==Ydim//1024:\n",
    "                    nxmin = ext[1] - 1024 * 0.6\n",
    "                    nxmax = ext[1]\n",
    "                    nymin = ext[2]\n",
    "                    nymax = ext[2] + 1024 * 0.6\n",
    "                    #pxlim = [Xdim - 1024, Xdim, Ydim - 1024, Ydim]\n",
    "                \n",
    "            else:\n",
    "                nxmin = ext[0] + 1024 * 0.6 * i\n",
    "                nxmax = ext[0] + 1024 * 0.6 * (i+1)\n",
    "                nymin = ext[3] - 1024 * 0.6 * (j+1)\n",
    "                nymax = ext[3] - 1024 * 0.6 * j\n",
    "                #pxlim = [1024*i, 1024*(i+1), 1024*j, 1024*(j+1)]\n",
    "                \n",
    "            !gdalwarp -overwrite -te $nxmin $nymin $nxmax $nymax $file_path $final_path\n",
    "            !gdalwarp -overwrite -te $nxmin $nymin $nxmax $nymax $gt_path $final_gt_path\n",
    "            \n",
    "            #json_path = os.path.join(dataset_dir, name + '.json')\n",
    "            #final_json_path = os.path.join(final_dir, name[:-2]+ '_'+str(i)+str(j)+'.json')\n",
    "            #CropAnnot(json_path, final_json_path, final_path, final_name, pxlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2246bcc-2280-4b87-84bc-dad107453e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e676707-69e8-4ff8-ad59-d6c0ab3929d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp path is C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\\tmpINRIA\n"
     ]
    }
   ],
   "source": [
    "tmp_path = os.path.join(dataset_dir, 'tmpINRIA')\n",
    "print('tmp path is {}'.format(tmp_path))\n",
    "preprocess_path = os.path.join(dataset_dir, 'preprocessINRIA')\n",
    "if not os.path.isdir(preprocess_path):\n",
    "    os.mkdir(preprocess_path)\n",
    "    os.mkdir(os.path.join(preprocess_path, 'images'))\n",
    "    os.mkdir(os.path.join(preprocess_path, 'gt'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae7b1986-930a-475a-b1e0-d8a76bed203e",
   "metadata": {},
   "source": [
    "lst = os.listdir(tmp_path)\n",
    "for geo in lst: # 1 GeoTIFF preprocessed every 5 elements\n",
    "    if geo.endswith('.tif'):\n",
    "        with open(os.path.join(dataset_dir,'del.txt')) as file:\n",
    "            lines = file.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "            if not any([geo.startswith(sub) for sub in lines]): \n",
    "                # si le nom du fichier ne commence par aucun des attributs de del.txt, alors on le préprocesse\n",
    "                crop(tmp_path, preprocess_path, geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611164b-05f4-48c7-9d1b-9409a3bf1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(os.path.join(tmp_path, 'images'))\n",
    "for geo in lst:\n",
    "    crop(tmp_path, preprocess_path, geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783207d-8e87-4688-80e7-9ca0c901afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(preprocess_path, 'images', 'vienna15_02.tif')\n",
    "!gdalinfo $filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30746-459f-4a17-afad-24aa8c2e4f34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transform Annotations from TIF binary tiles to VGG annotation tool json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e19634-6b95-4f7d-8996-f42f8435caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VArri\\Documents\\GitHub\\RoofTop\\samples\\darkroof\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb89a3e-bc67-4c1d-a038-e4fc0d1bc2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\\preprocessINRIA\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2cc10-5ad0-4d63-8363-22ab5bd3955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bb2c5d-d58b-43de-a445-446470c0af24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random \n",
    "def Annot(preprocess_path, res=0.6, surf=100, eps=0.01):\n",
    "    \n",
    "    john=True\n",
    "    stats=[]\n",
    "    jsonf = {} # only one big annotation file\n",
    "    \n",
    "    with open(os.path.join(preprocess_path,'via_region_data.json'), 'w') as js_file:\n",
    "        gt_path = os.path.join(preprocess_path, 'gt')\n",
    "        images_path = os.path.join(preprocess_path, 'images')\n",
    "        \n",
    "        # All the elements in the images folders\n",
    "        lst = os.listdir(images_path)\n",
    "        for elt in tqdm(lst, desc='lst'):\n",
    "        \n",
    "            # Read the binary mask, and find the contours associated\n",
    "            gray = cv2.imread(os.path.join(gt_path, elt))\n",
    "            imgray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "            contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Only keep contours with a minimum surface to avoid overload in preprocessing\n",
    "            # Also mandatory to reduce the length of contours\n",
    "            # https://www.pyimagesearch.com/2021/10/06/opencv-contour-approximation/\n",
    "            # Contours approximation based on Ramer–Douglas–Peucker (RDP) algorithm\n",
    "            areas = [cv2.contourArea(contours[idx])*res*res for idx in range(len(contours))]\n",
    "            large_contour = []\n",
    "            for i in range(len(areas)):\n",
    "                if areas[i]>surf:\n",
    "                    large_contour.append(contours[i])\n",
    "            # large_contour = [contours[i] if areas[i]>surf else contours[0] for i in range(len(areas))]\n",
    "            approx_contour = [cv2.approxPolyDP(c, eps * cv2.arcLength(c, True), True) for c in large_contour]\n",
    "            # print('Taille des contours larges {}'.format([len(large_contour[i]) for i in range(len(large_contour))]))\n",
    "            # print('Taille des contours approx {}'.format([len(approx_contour[i]) for i in range(len(approx_contour))]))\n",
    "            # Stats on ApproxPolyDP to approximate teh contour, in particular the number of points\n",
    "            for idx in range(len(large_contour)):\n",
    "                stats.append((len(large_contour[idx])-len(approx_contour[idx]))/len(large_contour[idx]))\n",
    "            \n",
    "            if False: # random.randint(0,5)==3:\n",
    "\n",
    "                im = cv2.imread(os.path.join(images_path, elt))\n",
    "                cv2.drawContours(im, large_contour, -1, (0,255,0), 3)\n",
    "                # cv2.drawContours(im, approx_contour, -1, (0,255,0), 3)\n",
    "                cv2.imshow('contours', im)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "            \n",
    "            #print(approx_contour)\n",
    "            #print(approx_contour[0])\n",
    "            #print(approx_contour[0][:,0])\n",
    "            #print(approx_contour[0][:,0][:,1])\n",
    "            \n",
    "            # -------------------------------------------------------------------------------\n",
    "            # BUILDING VGG ANNTOTATION TOOL ANNOTATIONS LIKE \n",
    "            if len(approx_contour) > 0:\n",
    "                \n",
    "                                    \n",
    "                regions = [0 for i in range(len(approx_contour))]\n",
    "                for i in range(len(approx_contour)):\n",
    "                    # print('ok')\n",
    "                    # print(i)\n",
    "                    shape_attributes = {}\n",
    "                    region_attributes = {}\n",
    "                    regionsi = {}\n",
    "                    shape_attributes['name'] = 'polygon'\n",
    "                    shape_attributes['all_points_x'] = approx_contour[i][:, 0][:, 0].tolist()\n",
    "                    # https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "                    shape_attributes['all_points_y'] = approx_contour[i][:, 0][:, 1].tolist()\n",
    "                    # print(json.dumps(shape_attributes, indent=4))\n",
    "                    regionsi['shape_attributes'] = shape_attributes\n",
    "                    regionsi['region_attributes'] = region_attributes\n",
    "                    regions[i] = regionsi\n",
    "\n",
    "                size = os.path.getsize(os.path.join(images_path, elt))\n",
    "                name = elt + str(size)\n",
    "                json_elt = {}\n",
    "                json_elt['filename'] = elt\n",
    "                json_elt['size'] = str(size)\n",
    "                json_elt['regions'] = regions\n",
    "                json_elt['file_attributes'] = {}\n",
    "                # print(json.dumps(json_elt, indent=4))\n",
    "\n",
    "                jsonf[name] = json_elt\n",
    "        \n",
    "                \n",
    "            if len(approx_contour) ==3 and john:\n",
    "                print(json.dumps(jsonf, indent=4))\n",
    "                john = False\n",
    "                \n",
    "        json.dump(jsonf, js_file) ### EXPLAIN A LITTLE BIT MORE\n",
    "    \n",
    "    # Stats processing \n",
    "    print('ApproxPolyDP is decreasing contours number of points by {} % on average'.format(np.around(np.mean(stats)*100),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61f60eb-3ac1-4781-9c14-7187313863f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lst:  71%|████████████████████████████████████████████████████▉                      | 185/262 [00:06<00:05, 13.51it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "lst: 100%|███████████████████████████████████████████████████████████████████████████| 262/262 [00:08<00:00, 30.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApproxPolyDP is decreasing contours number of points by 69.0 % on average\n"
     ]
    }
   ],
   "source": [
    "Annot(os.path.join(preprocess_path, 'colab', 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d21be5e-96f3-4e61-be7b-086dbe925ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data['0030fd0e6378.png236322']['regions'][0]['shape_attributes']['all_points_x'][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f83ff29f-4398-43a3-9e68-8fba3d5864ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AEC255F4-7883-4EE9-9DDB-F8216AD6613F.png9259408\": {\n",
      "        \"filename\": \"AEC255F4-7883-4EE9-9DDB-F8216AD6613F.png\",\n",
      "        \"size\": 9259408,\n",
      "        \"regions\": [\n",
      "            {\n",
      "                \"shape_attributes\": {\n",
      "                    \"name\": \"polygon\",\n",
      "                    \"all_points_x\": [\n",
      "                        291,\n",
      "                        531,\n",
      "                        536,\n",
      "                        298,\n",
      "                        230\n",
      "                    ],\n",
      "                    \"all_points_y\": [\n",
      "                        696,\n",
      "                        686,\n",
      "                        787,\n",
      "                        797,\n",
      "                        898\n",
      "                    ]\n",
      "                },\n",
      "                \"region_attributes\": {}\n",
      "            }\n",
      "        ],\n",
      "        \"file_attributes\": {}\n",
      "    },\n",
      "    \"0030fd0e6378.png236322\": {\n",
      "        \"filename\": \"0030fd0e6378.png\",\n",
      "        \"size\": 236322,\n",
      "        \"regions\": [\n",
      "            {\n",
      "                \"shape_attributes\": {\n",
      "                    \"name\": \"polygon\",\n",
      "                    \"all_points_x\": [\n",
      "                        291,\n",
      "                        461,\n",
      "                        137,\n",
      "                        156\n",
      "                    ],\n",
      "                    \"all_points_y\": [\n",
      "                        190,\n",
      "                        256,\n",
      "                        330,\n",
      "                        174\n",
      "                    ]\n",
      "                },\n",
      "                \"region_attributes\": {}\n",
      "            },\n",
      "            {\n",
      "                \"shape_attributes\": {\n",
      "                    \"name\": \"polygon\",\n",
      "                    \"all_points_x\": [\n",
      "                        371,\n",
      "                        487,\n",
      "                        637,\n",
      "                        362\n",
      "                    ],\n",
      "                    \"all_points_y\": [\n",
      "                        350,\n",
      "                        276,\n",
      "                        394,\n",
      "                        450\n",
      "                    ]\n",
      "                },\n",
      "                \"region_attributes\": {}\n",
      "            },\n",
      "            {\n",
      "                \"shape_attributes\": {\n",
      "                    \"name\": \"polygon\",\n",
      "                    \"all_points_x\": [\n",
      "                        374,\n",
      "                        596,\n",
      "                        234,\n",
      "                        341,\n",
      "                        306,\n",
      "                        368\n",
      "                    ],\n",
      "                    \"all_points_y\": [\n",
      "                        38,\n",
      "                        196,\n",
      "                        105,\n",
      "                        49,\n",
      "                        30,\n",
      "                        5\n",
      "                    ]\n",
      "                },\n",
      "                \"region_attributes\": {}\n",
      "            }\n",
      "        ],\n",
      "        \"file_attributes\": {}\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "jsonpath = r\"C:\\Users\\VArri\\Downloads\"\n",
    "with open(os.path.join(jsonpath, 'via_export_json.json'), 'r') as op:\n",
    "    data = json.load(op)\n",
    "    print(json.dumps(data, indent=4))\n",
    "    #for i in data['0030fd0e6378.png236322']:\n",
    "        # print(i)\n",
    "        # pour lire les infos directement dans les paniers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55d07f-391d-4b18-9656-6fb7778475b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shuffling and split annotated files in train, val, test folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28604cf5-b277-4168-940b-b7f340658c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\VArri\\\\Documents\\\\Rooftop\\\\dataset\\\\dataset\\\\dataset'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53b3e47-e415-4639-8cf3-4c6bfc9e2d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lst: 100%|█████████████████████████████████████████████████████████████████████████| 1296/1296 [01:19<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "\n",
    "colab_path = os.path.join(dataset_dir, 'preprocessINRIA', 'colab')\n",
    "if not os.path.isdir(colab_path):\n",
    "    os.mkdir(colab_path)\n",
    "    os.mkdir(os.path.join(colab_path, 'train'))\n",
    "    os.mkdir(os.path.join(colab_path, 'train', 'images'))\n",
    "    os.mkdir(os.path.join(colab_path, 'train', 'gt'))\n",
    "    os.mkdir(os.path.join(colab_path, 'val'))\n",
    "    os.mkdir(os.path.join(colab_path, 'val', 'images'))\n",
    "    os.mkdir(os.path.join(colab_path, 'val', 'gt'))\n",
    "    # os.mkdir(os.path.join(colab_path, 'test'))\n",
    "    #os.mkdir(os.path.join(colab_path, 'test', 'images'))\n",
    "    #os.mkdir(os.path.join(colab_path, 'test', 'gt'))\n",
    "                          \n",
    "    \n",
    "from shutil import copyfile\n",
    "img_path = os.path.join(dataset_dir, 'preprocessINRIA', 'images')\n",
    "gt_path = os.path.join(dataset_dir, 'preprocessINRIA', 'gt')\n",
    "lst = os.listdir(img_path)\n",
    "shuffle(lst)\n",
    "for geo in tqdm(lst, desc='lst'):\n",
    "    if geo.endswith('.tif'):\n",
    "        name, exte = geo.split('.')\n",
    "        if True:#os.path.isfile(os.path.join(preprocess_path, name+'.json')):\n",
    "            var = randint(0, 9)\n",
    "            if var in range(0,8):\n",
    "                folder = 'train'                \n",
    "            elif var in range(8,10):\n",
    "                folder = 'val'            \n",
    "            # elif var==0:\n",
    "            #     folder = 'test'\n",
    "                \n",
    "            copyfile(os.path.join(img_path,geo), os.path.join(colab_path,folder,'images', geo))\n",
    "            copyfile(os.path.join(gt_path,geo), os.path.join(colab_path,folder, 'gt', geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163abda-e0cb-45df-8f36-746bdc5dffe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
