{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cbc00f-5510-4208-bc9f-4f94898a3c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing des images .TIF du dataset lnria Aerial Image Labeling Dataset\n",
    "\n",
    "5000x5000 px en résolution 0.3 m /px vers 1024x1024 px en résolution 0.6 m /px\n",
    "\n",
    "seulement les images sélectionnées à l'oeil dans le fichier prepro.txt selon les critères suivants :\n",
    "- toit plat \n",
    "- toit gris sombre ou noir \n",
    "\n",
    "iteration 24/08\n",
    "- faire la conversion en resolution 0.6m/px\n",
    "- faire l'annotation\n",
    "- découper les images avec gdalwarp, également les annotations json\n",
    "- afin de renverser l'ordre découpage annotations pour gagner du temps et annoter directement les grandes images\n",
    "  - problème : presque impossible de créer de nouveaux fichiers d'annotations à la main en particulier la section 'imageData' du fichier d'annotation .json\n",
    "\n",
    "itération 30/08\n",
    "- faire la conversion en résolution 0.6m/px dans le fichier destination 'tmp'\n",
    "- dans le dossier 'tmp', écrire dans un fichier texte celles qui ne sont pas exploitables\n",
    "- découper celles qui ne sont pas listées dans le fichier dans le fichier texte dans le dossier 'preprocess'\n",
    "- annoter de façon aléatoire un certain nombre d'images\n",
    "- copier les images annotées + les annotations en les splittant train + val + test dans le dossier 'colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1ce35c-b04e-4c45-978b-37690e8cecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from osgeo import gdal \n",
    "import json\n",
    "import copy\n",
    "import base64\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39c0f23-18be-448b-8dc8-6a0425c0a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\"\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train', 'images')\n",
    "test_dir  = os.path.join(dataset_dir, 'test', 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ae114-295c-4f6c-9eed-99044a7d8607",
   "metadata": {},
   "source": [
    "## Convert GeoTIFF to 0.6 m resolution GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20ac6a-70b7-425e-8f3a-8d1ac976651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(dataset_dir, res_dir, file_name, res=0.6):\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    name, ext = file_name.split('.')\n",
    "    res_path = os.path.join(res_dir, name+'_s.'+ext)\n",
    "    !gdalwarp -tr $res $res $file_path $res_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2427dc-bd98-403b-ad95-520a69535000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = os.path.join(dataset_dir, 'tmp')\n",
    "if not os.path.isdir(tmp_path):\n",
    "    os.mkdir(tmp_path)\n",
    "\n",
    "lst = os.listdir(train_dir)\n",
    "for i in lst:\n",
    "    if i.startswith('austin') or i.startswith('chicago') or i.startswith('vienna'):\n",
    "        res(train_dir, tmp_path, i, res=0.6)\n",
    "        \n",
    "test_lst = os.listdir(test_dir)\n",
    "for j in test_lst:\n",
    "    if j.startswith('bellingham') or j.startswith('bloomington') or j.startswith('sfo'):\n",
    "        res(test_dir, tmp_path, j, res=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dbd5a-6d00-440a-8593-674ccc0bdfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7752b047-e540-417c-ab8c-6bc98836da85",
   "metadata": {},
   "source": [
    "## Convert GeoTIFF to 1024 px side GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b937fe11-f0f1-4c2f-bf2b-d82593113746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetExtent(ds):\n",
    "    \"\"\" Return list of corner coordinates from a gdal Dataset \"\"\"\n",
    "    xmin, xpixel, _, ymax, _, ypixel = ds.GetGeoTransform()\n",
    "    width, height = ds.RasterXSize, ds.RasterYSize\n",
    "    xmax = xmin + width * xpixel\n",
    "    ymin = ymax + height * ypixel\n",
    "    return round(xmin,0), round(xmax,0), round(ymin,0), round(ymax, 0)\n",
    "# https://gis.stackexchange.com/questions/57834/how-to-get-raster-corner-coordinates-using-python-gdal-bindings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "064973cf-d0cb-4739-8277-66f6a595114c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def CropAnnot(json_path, final_json_path, final_path, final_name, pxlim):\n",
    "    # réalise le recadrage pour les annotations dans le repère local\n",
    "    with open(json_path) as inpt:\n",
    "        data = json.load(inpt)\n",
    "            \n",
    "        lst = {}\n",
    "        lst['version']= data['version']\n",
    "        lst['flags']=   data['flags']\n",
    "        lst['shapes']=  []\n",
    "        #print('LST {}'.format(lst))\n",
    "        #print('SHAPES ARE {}'.format(data['shapes']))\n",
    "        for i in range(len(data['shapes'])):\n",
    "            # print(i)\n",
    "            shp = data['shapes'][i]\n",
    "            if not any([not (pxlim[0]<p[0]<pxlim[1] and pxlim[2]<p[1]<pxlim[3]) for p in shp['points']]): \n",
    "                # si tous les points sont dans le recadrage\n",
    "                # retrancher les pixels au points d'annotations\n",
    "                for k in range(len(shp['points'])):\n",
    "                    shp['points'][k][0] = shp['points'][k][0] - pxlim[0]\n",
    "                    shp['points'][k][1] = shp['points'][k][1] - pxlim[2]\n",
    "                lst['shapes'].append(shp)\n",
    "        lst['imagePath'] = final_name\n",
    "        # lst['imageData'] = base64.b64encode(open(final_path, \"rb\").read()) \n",
    "            # https://stackoverflow.com/questions/57004792/what-is-imagedata-in-json-file-which-comes-from-labelme-tool\n",
    "            # https://github.com/wkentaro/labelme/issues/389  remove imageData ? \n",
    "            # json se charge de bianriser le fichier image ? \n",
    "            # https://stackoverflow.com/questions/56428037/what-is-the-best-way-to-save-pil-image-in-json\n",
    "            # hdancrafting labelme json annotation file is too difficult :-( \n",
    "        #lst['imageData'] = Image.open(final_path).tobytes().decode('ascii')  \n",
    "        lst['imageData'] = \"hucezrhirehuihviuerniiu\"\n",
    "        lst['imageHeight'], lst['imageWidth'] = 1024,  1024\n",
    "            \n",
    "        # copie conforme de json, lit dans un, supprime dans l'autre pour éviter pb bidon de décalage,\n",
    "        # en fait non, on reste sur un seul fichier json mais on le parcourt à l'envers dans le sens des entiers décroissant\n",
    "        # en fait re-non , ca ne résout toujours pas le pb, plutôt on va créer une copie conforme que l'on va remplir au fur et à mesure \n",
    "        if not len(lst['shapes'])<1:\n",
    "            with open(final_json_path, 'w+') as outpt:\n",
    "                json.dump(lst, outpt, indent=2)\n",
    "                # pas besoin d'écrire de fichier d'annotations s'il n'y pas d'annotations à l'intérieur\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6215ac13-e08e-4081-a130-11fa26356b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dataset_dir, final_dir, file_name):\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    \n",
    "    name, exte = file_name.split('.')\n",
    "    raster = gdal.Open(file_path)\n",
    "    # print(file_path)\n",
    "    ext = GetExtent(raster)\n",
    "    Xdim, Ydim = raster.RasterXSize, raster.RasterYSize\n",
    "    for i in range(Xdim//1024+1):\n",
    "        for j in range(Ydim//1024+1):\n",
    "            final_name = name[:-2]+ '_'+str(i)+str(j)+'.'+exte\n",
    "            final_path = os.path.join(final_dir, final_name)\n",
    "            \n",
    "            if i==Xdim//1024 or j==Ydim//1024:\n",
    "                if i==Xdim//1024 and j!=Ydim//1024:\n",
    "                    nxmin = ext[1] - 1024 * 0.6\n",
    "                    nxmax = ext[1]\n",
    "                    nymin = ext[3] - 1024 * 0.6 * (j+1)\n",
    "                    nymax = ext[3] - 1024 * 0.6 * j\n",
    "                    #pxlim = [Xdim - 1024, Xdim, 0, 1024]\n",
    "                    \n",
    "                elif i!=Xdim//1024 and j==Ydim//1024:\n",
    "                    nxmin = ext[0] + 1024 * 0.6 * i\n",
    "                    nxmax = ext[0] + 1024 * 0.6 * (i+1)\n",
    "                    nymin = ext[2]\n",
    "                    nymax = ext[2] + 1024 * 0.6\n",
    "                    #pxlim = [0, 1024, Ydim - 1024, Ydim]\n",
    "                    \n",
    "                elif i==Xdim//1024 and j==Ydim//1024:\n",
    "                    nxmin = ext[1] - 1024 * 0.6\n",
    "                    nxmax = ext[1]\n",
    "                    nymin = ext[2]\n",
    "                    nymax = ext[2] + 1024 * 0.6\n",
    "                    #pxlim = [Xdim - 1024, Xdim, Ydim - 1024, Ydim]\n",
    "                \n",
    "            else:\n",
    "                nxmin = ext[0] + 1024 * 0.6 * i\n",
    "                nxmax = ext[0] + 1024 * 0.6 * (i+1)\n",
    "                nymin = ext[3] - 1024 * 0.6 * (j+1)\n",
    "                nymax = ext[3] - 1024 * 0.6 * j\n",
    "                #pxlim = [1024*i, 1024*(i+1), 1024*j, 1024*(j+1)]\n",
    "                \n",
    "            !gdalwarp -overwrite -te $nxmin $nymin $nxmax $nymax $file_path $final_path\n",
    "            \n",
    "            #json_path = os.path.join(dataset_dir, name + '.json')\n",
    "            #final_json_path = os.path.join(final_dir, name[:-2]+ '_'+str(i)+str(j)+'.json')\n",
    "            #CropAnnot(json_path, final_json_path, final_path, final_name, pxlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2246bcc-2280-4b87-84bc-dad107453e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e676707-69e8-4ff8-ad59-d6c0ab3929d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp path is C:\\Users\\VArri\\Documents\\Rooftop\\dataset\\dataset\\dataset\\tmp\n"
     ]
    }
   ],
   "source": [
    "tmp_path = os.path.join(dataset_dir, 'tmp')\n",
    "print('tmp path is {}'.format(tmp_path))\n",
    "preprocess_path = os.path.join(dataset_dir, 'preprocess')\n",
    "if not os.path.isdir(preprocess_path):\n",
    "    os.mkdir(preprocess_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5119f0-2be6-4ab1-b434-10a6adce4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(tmp_path)\n",
    "for geo in lst: # 1 GeoTIFF preprocessed every 5 elements\n",
    "    if geo.endswith('.tif'):\n",
    "        with open(os.path.join(dataset_dir,'del.txt')) as file:\n",
    "            lines = file.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "            if not any([geo.startswith(sub) for sub in lines]): \n",
    "                # si le nom du fichier ne commence par aucun des attributs de del.txt, alors on le préprocesse\n",
    "                crop(tmp_path, preprocess_path, geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783207d-8e87-4688-80e7-9ca0c901afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "filepath = os.path.join(tmp_path, 'austin16_s.tif')\n",
    "!gdalinfo $filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55d07f-391d-4b18-9656-6fb7778475b8",
   "metadata": {},
   "source": [
    "# Shuffling and split annotated files in train, val, test folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53b3e47-e415-4639-8cf3-4c6bfc9e2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "colab_path = os.path.join(dataset_dir, 'colab2')\n",
    "if not os.path.isdir(colab_path):\n",
    "    os.mkdir(colab_path)\n",
    "    os.mkdir(os.path.join(colab_path, 'train'))\n",
    "    os.mkdir(os.path.join(colab_path, 'val'))\n",
    "    os.mkdir(os.path.join(colab_path, 'test'))\n",
    "    \n",
    "from shutil import copyfile\n",
    "lst = os.listdir(preprocess_path)\n",
    "shuffle(lst)\n",
    "for geo in lst:\n",
    "    if geo.endswith('.tif'):\n",
    "        name, exte = geo.split('.')\n",
    "        if os.path.isfile(os.path.join(preprocess_path, name+'.json')):\n",
    "            \n",
    "            if lst.index(geo)%10 in range(1,8):\n",
    "                folder = 'train'                \n",
    "            elif lst.index(geo)%10 in range(8,10):\n",
    "                folder = 'val'            \n",
    "            elif lst.index(geo)%10==0:\n",
    "                folder = 'test'\n",
    "                \n",
    "            copyfile(os.path.join(preprocess_path,geo), os.path.join(colab_path,folder,geo))\n",
    "            copyfile(os.path.join(preprocess_path,name+'.json'), os.path.join(colab_path,folder,name+'.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ff29f-4398-43a3-9e68-8fba3d5864ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
